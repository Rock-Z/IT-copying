Transformers version: 4.45.2
PyTorch version: 2.5.0+cu124
Available GPUs: 2
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:06<00:33,  6.63s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:13<00:26,  6.70s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:19<00:19,  6.66s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:26<00:13,  6.57s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:27<00:04,  4.72s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  3.21s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.69s/it]
Some parameters are on the meta device because they were offloaded to the cpu.
Generating for dataset bbc:   0%|          | 0/500 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Generating for dataset bbc:   0%|          | 0/500 [00:29<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/src/generate.py", line 176, in <module>
    main()
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/src/generate.py", line 170, in main
    outputs[key][idx] = run_generation(model, tokenizer, prompt)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/src/generate.py", line 123, in run_generation
    outputs = model.generate(inputs, **generate_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/transformers/generation/utils.py", line 2047, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/transformers/generation/utils.py", line 3007, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/transformers/models/olmo/modeling_olmo.py", line 1132, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/gpfs/gibbs/project/mccoy/ez275/IT-Copying/.venv/lib64/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 349.12 MiB is free. Including non-PyTorch memory, this process has 10.23 GiB memory in use. Of the allocated memory 9.26 GiB is allocated by PyTorch, and 803.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  In call to configurable 'run_generation' (<function run_generation at 0x14abdcf20cc0>)
